{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from backbone.mobilenetv3 import mobilenetv3_large\n",
    "from backbones.mobilenetv3_pytorch import mobilenet_v3_large\n",
    "from backbones.mobilenetv3_pytorch import mobilenet_v3_small\n",
    "\n",
    "from backbones import resnet\n",
    "from backbones import regnet\n",
    "from backbones import efficientnet\n",
    "\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score\n",
    "import scikitplot as skplt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vairiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data=\"/media/msi/Silicon/project/person_lying_down/data_set/trranning_sessions/№2_feb_24_2022/\"\n",
    "\n",
    "\n",
    "\n",
    "weights=\"/media/msi/Silicon/project/person_lying_down/train_scripts/train_image_classifer_optuna/RESULTS/lying_ornot_2022-03-01_17-47-01_resnet18/w0d1o97p_btach=1568_lr=0.00212_epochs=160/checkpoint_0001.pth\"\n",
    "# weights=\"/media/msi/Silicon/project/person_lying_down/train_scripts/train_image_classifer_optuna/RESULTS/lying_ornot_2022-03-22_15-59-24_resnet18/ougz1eor_batch=448_lr=0.00281_epochs=80_resolush=336/checkpoint_0000.pth\"\n",
    "\n",
    "\n",
    "# '''\n",
    "# ['mobilenet_v3_large','mobilenet_v3_small','mobilenet_v2', \n",
    "# 'resnet18', 'resnet34', 'resnet50',\n",
    "# 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2',\n",
    "# 'regnet_x_400mf','regnet_x_800mf','regnet_x_1_6gf','regnet_x_3_2gf']\n",
    "# '''\n",
    "\n",
    "\n",
    "batch_train=64\n",
    "batch_valid=64\n",
    "batch_test=64\n",
    "\n",
    "# device = \"cuda:0\"\n",
    "\n",
    "csv_append=\"/media/msi/Silicon/project/person_lying_down/train_scripts/train_image_classifer_optuna/RESULTS/summarize.csv\" # path for saving all results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dict for summarize experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolush 224\n",
      "backbone resnet18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hash_exprmnt': None,\n",
       " 'backbone': 'resnet18',\n",
       " 'batch_size': 1568.0,\n",
       " 'lr': 0.00212,\n",
       " 'epochs': 160.0,\n",
       " 'resolush': 224.0,\n",
       " 'Loss_train': None,\n",
       " 'Accuracy_train': None,\n",
       " 'Precision_train': None,\n",
       " 'f1_score_train': None,\n",
       " 'Time_train': None,\n",
       " 'infer_batch_train': None,\n",
       " 'FPS_train': None,\n",
       " 'Loss_valid': None,\n",
       " 'Accuracy_valid': None,\n",
       " 'Precision_valid': None,\n",
       " 'f1_score_valid': None,\n",
       " 'Time_valid': None,\n",
       " 'infer_batch_valid': None,\n",
       " 'FPS_valid': None,\n",
       " 'Loss_test': None,\n",
       " 'Accuracy_test': None,\n",
       " 'Precision_test': None,\n",
       " 'f1_score_test': None,\n",
       " 'Time_test': None,\n",
       " 'infer_batch_test': None,\n",
       " 'FPS_test': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define init colums\n",
    "\n",
    "colums=\"hash_exprmnt;backbone;batch_size;lr;epochs;resolush;Loss_train;Accuracy_train;Precision_train;f1_score_train;Time_train;infer_batch_train;FPS_train;Loss_valid;Accuracy_valid;Precision_valid;f1_score_valid;Time_valid;infer_batch_valid;FPS_valid;Loss_test;Accuracy_test;Precision_test;f1_score_test;Time_test;infer_batch_test;FPS_test\"\n",
    "colums_dict=dict()\n",
    "for i in colums.split(\";\"):\n",
    "    colums_dict[i]=None\n",
    "colums_dict\n",
    "\n",
    "# Create csv-file for summarize\n",
    "if not os.path.isfile(csv_append):\n",
    "    with open(csv_append, \"w\") as file:\n",
    "        file.write(colums+'\\n')\n",
    "\n",
    "# Read csv experiment and turn it into dictionary\n",
    "param_exp=os.path.join(weights[:weights.rfind(\"/\")],\"param_expirement.csv\")\n",
    "df = pd.read_csv (param_exp,sep=\"\\t\") #\n",
    "param_exp=dict()\n",
    "for i in df.columns[1:]:\n",
    "    param_exp[i]=df.iloc[0][i]\n",
    "\n",
    "# Add from csv filex experiments-trial to dictinary    \n",
    "for i in param_exp:\n",
    "    colums_dict[i]=param_exp[i]\n",
    "colums_dict\n",
    "\n",
    "\n",
    "resolush=int(colums_dict['resolush'])\n",
    "backbone=colums_dict['backbone']\n",
    "\n",
    "print(\"resolush\",resolush)\n",
    "print(\"backbone\",backbone)\n",
    "\n",
    "colums_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Device check ########\n",
    "\n",
    "if not 'device' in locals():\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Create classes ########\n",
    "\n",
    "classes = os.listdir(path_data)\n",
    "classes.sort()\n",
    "classes\n",
    "print(f'Classes : {classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########  Choose backone ########\n",
    "\n",
    "if backbone == 'mobilenet_v2':\n",
    "    model = mobilenet_v2(pretrained=False)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(classes))\n",
    "elif backbone == 'mobilenet_v3_large':\n",
    "    model = mobilenet_v3_large()\n",
    "    model.load_state_dict(torch.load('./pretrain_weight/mobilenet_v3_large-8738ca79.pth'))\n",
    "    model.classifier[3]=nn.Linear(model.classifier[3].in_features, len(classes))\n",
    "elif backbone == 'mobilenet_v3_small':\n",
    "    model = mobilenet_v3_small()\n",
    "    model.load_state_dict(torch.load('./pretrain_weight/mobilenet_v3_small-047dcff4.pth'))\n",
    "    model.classifier[3]=nn.Linear(model.classifier[3].in_features, len(classes))\n",
    "elif backbone in ['efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2']:\n",
    "    model = efficientnet.__dict__[backbone](pretrained=False)\n",
    "    model.classifier[1]=nn.Linear(in_features=model.classifier[1].in_features, out_features=len(classes), bias=True)\n",
    "elif backbone in ['regnet_x_400mf','regnet_x_800mf','regnet_x_1_6gf','regnet_x_3_2gf']:\n",
    "    model = regnet.__dict__[backbone](pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, out_features=len(classes), bias=True)\n",
    "elif backbone in ['resnet18', 'resnet34', 'resnet50']:\n",
    "    model = resnet.__dict__[backbone](pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "\n",
    "########  Load weights ########\n",
    "model.load_state_dict(torch.load(weights))\n",
    "model.to(device)\n",
    "model.share_memory()\n",
    "model.eval()\n",
    "\n",
    "# print(model)\n",
    "\n",
    "print('Backbone: {}'.format(backbone))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.1 Data_sets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, images: list, classes: list, use_albu: bool = False):\n",
    "        self.images = images\n",
    "        self.classes = classes\n",
    "        self.use_albu = use_albu\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor]:\n",
    "        img = Image.open(self.images[idx]).convert('RGB')\n",
    "        img = torchvision.transforms.Resize((resolush, resolush))(img)\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        img = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )(img)\n",
    "\n",
    "        c = os.path.dirname(self.images[idx]).split('/')[-2]\n",
    "\n",
    "        return img, self.images[idx], self.classes.index(c)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.2 Data_loader__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []\n",
    "valid_images = []\n",
    "test_images = []\n",
    "sub_dirs = ['train', 'valid', 'test']\n",
    "\n",
    "\n",
    "for c in classes:\n",
    "    for idx, sd in enumerate(sub_dirs):\n",
    "        tmp = glob.glob(f'{path_data}/{c}/{sd}/*.jpg')\n",
    "        tmp += glob.glob(f'{path_data}/{c}/{sd}/*.png')\n",
    "        tmp += glob.glob(f'{path_data}/{c}/{sd}/*.JPG')\n",
    "        tmp += glob.glob(f'{path_data}/{c}/{sd}/*.JPEG')\n",
    "        tmp += glob.glob(f'{path_data}/{c}/{sd}/*.PNG')\n",
    "\n",
    "        if idx == 0:\n",
    "            for t in tmp:\n",
    "                train_images.append(t)\n",
    "        elif idx == 1:\n",
    "            for t in tmp:\n",
    "                valid_images.append(t)\n",
    "        elif idx == 2:\n",
    "            for t in tmp:\n",
    "                test_images.append(t)\n",
    "                \n",
    "                \n",
    "train_dataset = DS(train_images, classes=classes, use_albu=False)\n",
    "valid_dataset = DS(valid_images, classes=classes, use_albu=False)\n",
    "test_dataset = DS(test_images, classes=classes, use_albu=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_train, shuffle=False,num_workers=16, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_valid, shuffle=False,num_workers=16, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_test, shuffle=False,num_workers=16, pin_memory=True)\n",
    "\n",
    "datalodaer_list={'test':test_dataloader,'valid':valid_dataloader,'train':train_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 Inference on all sets for test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.CrossEntropyLoss()\n",
    "sft_mx = nn.Softmax(dim=1)\n",
    "\n",
    "all_set_list_wrong={'test':[],'valid':[],'train':[]}\n",
    "all_set_list_right={'test':[],'valid':[],'train':[]}\n",
    "\n",
    "all_set_list_true={'test':[],'valid':[],'train':[]}\n",
    "all_set_list_pred={'test':[],'valid':[],'train':[]}\n",
    "\n",
    "\n",
    "for key, value in datalodaer_list.items():\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    \n",
    "    avg_prec = 0\n",
    "    avg_f1 = 0\n",
    "\n",
    "    list_wrong=[]\n",
    "    list_wrong_tmp=[]\n",
    "    list_wrong_prob=[]\n",
    "\n",
    "    list_right=[]\n",
    "    list_right_tmp=[]\n",
    "    list_right_prob=[]\n",
    "    \n",
    "    \n",
    "#     list_pred=[]\n",
    "#     list_true=[]\n",
    "    \n",
    "#     duration=timedelta(hours=0, minutes=0, seconds=0, microseconds=0)\n",
    "    duration=0\n",
    "    \n",
    "    for imgs, imgs_paths, labels in value:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            \n",
    "#             start = datetime.now().time()\n",
    "            out = model(imgs)\n",
    "#             end = datetime.now().time()\n",
    "            end_time = time.time()\n",
    "#             print(start_time,end_time)\n",
    "#             print(end_time-start_time, batch)\n",
    "#             print( (end_time-start_time)/batch)\n",
    "#             print( \"fps\", batch/(end_time-start_time))\n",
    "            \n",
    "            loss = criteria(out, labels)\n",
    "\n",
    "        # Count time duration    \n",
    "#         t1 = timedelta(hours=start.hour, minutes=start.minute, seconds=start.second,  microseconds=start.microsecond)\n",
    "#         t2 = timedelta(hours=end.hour,   minutes=end.minute,  seconds=end.second,  microseconds=end.microsecond) \n",
    "#         duration = duration+t2 - t1    \n",
    "        duration=duration+end_time-start_time\n",
    "        \n",
    "        avg_loss += round(loss.item(), 3)\n",
    "\n",
    "        tmp_sftmx_all=torch.max(sft_mx(out), dim=1)\n",
    "\n",
    "        out=tmp_sftmx_all[1].cpu().numpy()\n",
    "        tmp_sftmx=tmp_sftmx_all[0].cpu().numpy()\n",
    "        labels=labels.cpu().numpy()\n",
    "        \n",
    "        # Создаём словарь списков для ground_truth для каждого Dataset: train test valid есть ответы true и predict, содаём для confusion matrix\n",
    "        all_set_list_true[key]=all_set_list_true[key]+list(labels)\n",
    "        # Создаём словарь списков ответов модели для каждого Dataset: train test valid есть ответы true и predict, содаём для confusion matrix\n",
    "        all_set_list_pred[key]=all_set_list_pred[key]+list(out)\n",
    "\n",
    "        \n",
    "        # Создаём словарь с неправильными ответами записываю путь картинки вероятность классы\n",
    "        for wridx in np.where(out!=labels)[0]:\n",
    "            list_wrong_tmp.append({'true_label':labels[wridx],'model_answer':out[wridx],'path':imgs_paths[wridx],'prob':round(tmp_sftmx[wridx],3)})\n",
    "            list_wrong_prob.append(round(tmp_sftmx[wridx],3))\n",
    "            \n",
    "        # Создаём словарь с правильными ответами записываю путь картинки вероятность классы\n",
    "        for wridx in np.where(out==labels)[0]:\n",
    "            list_right_tmp.append({'true_label':labels[wridx],'model_answer':out[wridx],'path':imgs_paths[wridx],'prob':round(tmp_sftmx[wridx],3)})\n",
    "            list_right_prob.append(round(tmp_sftmx[wridx],3))\n",
    "\n",
    "        avg_acc += accuracy_score(labels, out)\n",
    "        avg_prec += precision_score(labels, out, average='macro')\n",
    "        avg_f1 += f1_score(labels, out, average='macro')\n",
    "        \n",
    "    \n",
    "    #Сортируем данные списки неправильных ответов по вероятности\n",
    "    for sort_prob in np.argsort(-1*np.array(list_wrong_prob)):\n",
    "        list_wrong.append(list_wrong_tmp[sort_prob])\n",
    "        \n",
    "    all_set_list_wrong[key]=list_wrong\n",
    "        \n",
    "    #Сортируем данные списки правильных ответов по вероятности\n",
    "    for sort_prob in np.argsort(-1*np.array(list_right_prob)):\n",
    "        list_right.append(list_right_tmp[sort_prob])\n",
    "        \n",
    "    all_set_list_right[key]=list_right\n",
    "   \n",
    "    if key==\"test\":\n",
    "        fps=len(test_dataset)/duration#.total_seconds()\n",
    "        batch=batch_test\n",
    "    elif key==\"valid\":\n",
    "        fps=len(valid_dataset)/duration#.total_seconds()\n",
    "        batch=batch_valid\n",
    "    elif key==\"train\":\n",
    "        fps=len(train_dataset)/duration#.total_seconds()\n",
    "        batch=batch_train\n",
    "    \n",
    "    print(\"Processing \"+key)\n",
    "    print('')\n",
    "    colums_dict['Loss_'+key]=round(avg_loss / len(value), 3)\n",
    "#     colums_dict['Accuracy_'+key]=round(avg_acc  / len(value), 3)\n",
    "    colums_dict['Accuracy_'+key]=accuracy_score(all_set_list_true[key], all_set_list_pred[key])\n",
    "#     colums_dict['Precision_'+key]=round(avg_prec  / len(value), 3)\n",
    "    colums_dict['Precision_'+key]=precision_score(all_set_list_true[key], all_set_list_pred[key], average='macro')\n",
    "#     colums_dict['f1_score_'+key]=round(avg_f1  / len(value), 3)\n",
    "    colums_dict['f1_score_'+key]=f1_score(all_set_list_true[key], all_set_list_pred[key], average='macro')\n",
    "    colums_dict['Time_'+key]=duration\n",
    "    colums_dict['infer_batch_'+key]=batch\n",
    "    colums_dict['FPS_'+key]=fps\n",
    "\n",
    "\n",
    "summarize=''\n",
    "for i in colums.split(\";\"):\n",
    "    print(i,\" : \",colums_dict[i])\n",
    "    summarize=summarize+str(colums_dict[i])+\";\"\n",
    "    \n",
    "with open(csv_append, \"a\") as file:\n",
    "    file.write(summarize+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(csv_append, sep=';',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics=pd.read_csv(csv_append, sep=';',index_col=False)[[\"hash_exprmnt\",\"backbone\",\"resolush\",\"Loss_train\",\"Accuracy_train\",\"Precision_train\",\"Loss_valid\",\"Accuracy_valid\",\"Precision_valid\",\"Loss_test\",\"Accuracy_test\",\"Precision_test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics=pd.read_csv(csv_append, sep=';',index_col=False)[[\"hash_exprmnt\",\"backbone\",\"resolush\",\"Loss_train\",\"Accuracy_train\",\"Precision_train\",\"Loss_valid\",\"Accuracy_valid\",\"Precision_valid\",\"Loss_test\",\"Accuracy_test\",\"Precision_test\"]]\n",
    "df_metrics = df_metrics.drop_duplicates(subset=['hash_exprmnt'], keep='last')\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perform=pd.read_csv(csv_append, sep=';',index_col=False)[[\"hash_exprmnt\",\"backbone\",\"resolush\",\"infer_batch_train\",\"FPS_train\",\"infer_batch_valid\",\"FPS_valid\",\"infer_batch_test\",\"FPS_test\"]]\n",
    "df_perform=df_perform.groupby(['hash_exprmnt','backbone','resolush']).mean()\n",
    "df_perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot_confusion_matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "\n",
    "for key, value in all_set_list_true.items():\n",
    "    tmp_true=[]\n",
    "    tmp_pred=[]\n",
    "    tmp_true = [classes[i] for i in all_set_list_true[key]]\n",
    "    tmp_pred = [classes[i] for i in all_set_list_pred[key]]\n",
    "    skplt.metrics.plot_confusion_matrix(tmp_true, tmp_pred,figsize=(8,8),title_fontsize=32,text_fontsize=32, title= 'Conf.mtrx. '+key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Вывод неправильно склассифицированых картинок, с сортировкой по убыванию веростностям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_incorrect(key_set, width_size):\n",
    "    for i in range(0, len(all_set_list_wrong[key_set]), width_size):\n",
    "        if i+width_size>len(all_set_list_wrong[key_set]):\n",
    "            yacheek=len(all_set_list_wrong[key_set])%width_size\n",
    "        else:\n",
    "            yacheek=width_size\n",
    "        fig, axes = plt.subplots(nrows = 1, ncols =yacheek,figsize = (15, 15) )\n",
    "        for idx, ax in enumerate(axes.flat):\n",
    "\n",
    "            tmp_path=all_set_list_wrong[key_set][i+idx]['path']\n",
    "            temp_prob=all_set_list_wrong[key_set][i+idx]['prob']\n",
    "            true_label=all_set_list_wrong[key_set][i+idx]['true_label']\n",
    "            model_answer=all_set_list_wrong[key_set][i+idx]['model_answer']\n",
    "            ax.set_title(f'true label:' +classes[true_label]+'\\n' \\\n",
    "                         f'pred label:'+classes[model_answer]+str(temp_prob)+'\\n' \\\n",
    "                         f'name_file '+tmp_path[1+tmp_path.rfind('/'):])\n",
    "            photo=plt.imread(tmp_path)\n",
    "            ax.imshow(photo)\n",
    "            ax.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_most_incorrect('test_set',3) # Подставить из сет: 'test_set','valid_set','train_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_correct(key_set, width_size):\n",
    "    for i in range(0, len(all_set_list_right[key_set]), width_size):\n",
    "        if i+width_size>len(all_set_list_right[key_set]):\n",
    "            yacheek=len(all_set_list_right[key_set])%width_size\n",
    "        else:\n",
    "            yacheek=width_size\n",
    "        fig, axes = plt.subplots(nrows = 1, ncols =yacheek,figsize = (15, 15) )\n",
    "        for idx, ax in enumerate(axes.flat):\n",
    "\n",
    "            tmp_path=all_set_list_right[key_set][i+idx]['path']\n",
    "            temp_prob=all_set_list_right[key_set][i+idx]['prob']\n",
    "            true_label=all_set_list_right[key_set][i+idx]['true_label']\n",
    "            model_answer=all_set_list_right[key_set][i+idx]['model_answer']\n",
    "            ax.set_title(f'true label:' +classes[true_label]+'\\n' \\\n",
    "                         f'pred label:'+classes[model_answer]+str(temp_prob)+'\\n' \\\n",
    "                         f'name_file '+tmp_path[1+tmp_path.rfind('/'):])\n",
    "            photo=plt.imread(tmp_path)\n",
    "            ax.imshow(photo)\n",
    "            ax.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_most_correct('test_set',4) # Подставить из сет: 'test_set','valid_set','train_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
